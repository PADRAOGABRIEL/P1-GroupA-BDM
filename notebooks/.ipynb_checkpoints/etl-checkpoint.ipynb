{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e3c9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports used across ETL exploration and final incremental pipeline\n",
    "import time\n",
    "import os, json, shutil\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.sql.functions import col, current_timestamp, input_file_name, to_date, unix_timestamp, broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35041a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized project paths for input, output, lookup, and ETL state\n",
    "BASE_PATH = \"/home/jovyan/work\"\n",
    "\n",
    "INBOX = f\"{BASE_PATH}/data/inbox\"\n",
    "OUTBOX = f\"{BASE_PATH}/data/outbox/trips_enriched.parquet\"\n",
    "LOOKUP = f\"{BASE_PATH}/data/taxi_zone_lookup.parquet\"\n",
    "STATE = f\"{BASE_PATH}/state/manifest.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68770396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read manifest file that tracks already processed input files\n",
    "def load_manifest(path):\n",
    "    if not os.path.exists(path):\n",
    "        return {\"processed_files\": []}\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13f1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist updated manifest to keep ingestion incremental and idempotent\n",
    "def save_manifest(path, manifest):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192bf9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['yellow_tripdata_2025-01.parquet', 'yellow_tripdata_2025-02.parquet'], [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute full input list and the subset of files that are new in this run\n",
    "manifest = load_manifest(STATE)\n",
    "processed = set(x[\"filename\"] for x in manifest[\"processed_files\"])\n",
    "\n",
    "all_files = sorted([f for f in os.listdir(INBOX) if f.endswith(\".parquet\")])\n",
    "new_files = [f for f in all_files if f not in processed]\n",
    "\n",
    "all_files, new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd55059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session for the ETL run\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Project1-ETL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddbb63",
   "metadata": {},
   "source": [
    "## ETL rules used in the final run\n",
    "\n",
    "This cell documents the business rules implemented in the final incremental Spark job.\n",
    "\n",
    "- **Cleaning rules**: keep only rows where required fields are non-null, `passenger_count > 0`, `trip_distance > 0`, and `tpep_dropoff_datetime > tpep_pickup_datetime`.\n",
    "- **Dedup key**: `VendorID`, `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `PULocationID`, `DOLocationID`.\n",
    "- **Incremental behavior**: process only files not present in `state/manifest.json`.\n",
    "- **Idempotency**: anti-join new batch against existing outbox using the dedup key, then append only unseen records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1451c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to verify whether output parquet folder already contains data files\n",
    "def outbox_has_data(path: str) -> bool:\n",
    "    return (\n",
    "        os.path.exists(path)\n",
    "        and os.path.isdir(path)\n",
    "        and any(name.startswith(\"part-\") and name.endswith(\".parquet\") for name in os.listdir(path))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a795b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup safety: remove empty/corrupted outbox folder before writing\n",
    "if os.path.exists(OUTBOX) and not outbox_has_data(OUTBOX):\n",
    "    print(\"OUTBOX exists but has no parquet parts. Deleting corrupted/empty outbox folder...\")\n",
    "    shutil.rmtree(OUTBOX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ed6132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files: ['yellow_tripdata_2025-01.parquet', 'yellow_tripdata_2025-02.parquet']\n",
      "New files: []\n"
     ]
    }
   ],
   "source": [
    "# Log file discovery result for this run (all vs new files)\n",
    "print(\"All files:\", all_files)\n",
    "print(\"New files:\", new_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb47438c-c65a-4129-9ae4-439a6d2f0f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing new to process. Exiting without changes.\n"
     ]
    }
   ],
   "source": [
    "# Final incremental ETL: process only new files, clean, dedup, enrich, append, and update manifest\n",
    "if not new_files:\n",
    "    print(\"Nothing new to process. Exiting without changes.\")\n",
    "else:\n",
    "    import time\n",
    "    start = time.time()\n",
    "    now = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    new_paths = [os.path.join(INBOX, f) for f in new_files]\n",
    "    df_new = spark.read.parquet(*new_paths).withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "    # ---------- Types ----------\n",
    "    df_typed = (\n",
    "        df_new\n",
    "        .withColumn(\"tpep_pickup_datetime\", col(\"tpep_pickup_datetime\").cast(\"timestamp\"))\n",
    "        .withColumn(\"tpep_dropoff_datetime\", col(\"tpep_dropoff_datetime\").cast(\"timestamp\"))\n",
    "        .withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"int\"))\n",
    "        .withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"double\"))\n",
    "        .withColumn(\"PULocationID\", col(\"PULocationID\").cast(\"int\"))\n",
    "        .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(\"int\"))\n",
    "        .withColumn(\"VendorID\", col(\"VendorID\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # ---------- Cleaning ----------\n",
    "    df_clean = df_typed.filter(\n",
    "        col(\"tpep_pickup_datetime\").isNotNull()\n",
    "        & col(\"tpep_dropoff_datetime\").isNotNull()\n",
    "        & col(\"passenger_count\").isNotNull()\n",
    "        & col(\"trip_distance\").isNotNull()\n",
    "        & col(\"PULocationID\").isNotNull()\n",
    "        & col(\"DOLocationID\").isNotNull()\n",
    "        & col(\"VendorID\").isNotNull()\n",
    "        & (col(\"passenger_count\") > 0)\n",
    "        & (col(\"trip_distance\") > 0)\n",
    "        & (col(\"tpep_dropoff_datetime\") > col(\"tpep_pickup_datetime\"))\n",
    "    )\n",
    "\n",
    "    # ---------- Dedup ----------\n",
    "    dedup_key = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"PULocationID\", \"DOLocationID\"]\n",
    "\n",
    "    # 1) remove duplicados dentro do batch\n",
    "    df_dedup_batch = df_clean.dropDuplicates(dedup_key)\n",
    "\n",
    "    # 2) opcional: remove registros já existentes no OUTBOX (idempotência mesmo sem manifest)\n",
    "    if outbox_has_data(OUTBOX):\n",
    "        existing_keys = (\n",
    "            spark.read.parquet(OUTBOX)\n",
    "            .select(*dedup_key)\n",
    "            .dropDuplicates(dedup_key)\n",
    "        )\n",
    "        df_dedup = df_dedup_batch.join(existing_keys, on=dedup_key, how=\"left_anti\")\n",
    "    else:\n",
    "        df_dedup = df_dedup_batch\n",
    "\n",
    "    # ---------- Enrichment ----------\n",
    "    zones = spark.read.parquet(LOOKUP)\n",
    "\n",
    "    zones_pickup = zones.select(\n",
    "        col(\"LocationID\").alias(\"PULocationID\"),\n",
    "        col(\"Zone\").alias(\"pickup_zone\")\n",
    "    )\n",
    "\n",
    "    zones_dropoff = zones.select(\n",
    "        col(\"LocationID\").alias(\"DOLocationID\"),\n",
    "        col(\"Zone\").alias(\"dropoff_zone\")\n",
    "    )\n",
    "\n",
    "    df_enriched = (\n",
    "        df_dedup\n",
    "        .join(broadcast(zones_pickup), on=\"PULocationID\", how=\"left\")\n",
    "        .join(broadcast(zones_dropoff), on=\"DOLocationID\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # ---------- Derived columns ----------\n",
    "    df_ready = (\n",
    "        df_enriched\n",
    "        .withColumn(\n",
    "            \"trip_duration_minutes\",\n",
    "            (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 60.0\n",
    "        )\n",
    "        .withColumn(\"pickup_date\", to_date(\"tpep_pickup_datetime\"))\n",
    "        .withColumn(\"ingested_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    # ---------- Select required output columns (helps grading) ----------\n",
    "    df_out = df_ready.select(\n",
    "        \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "        \"PULocationID\", \"DOLocationID\",\n",
    "        \"pickup_zone\", \"dropoff_zone\",\n",
    "        \"passenger_count\", \"trip_distance\",\n",
    "        \"trip_duration_minutes\", \"pickup_date\",\n",
    "        \"source_file\", \"ingested_at\"\n",
    "    )\n",
    "\n",
    "    # ---------- Metrics for README ----------\n",
    "    input_rows = df_new.count()\n",
    "    clean_rows = df_clean.count()\n",
    "    dedup_rows = df_dedup.count()\n",
    "    final_rows = df_out.count()  # should match dedup_rows in most cases\n",
    "\n",
    "    print(\"Row counts (new batch):\", {\n",
    "        \"input\": input_rows,\n",
    "        \"after_cleaning\": clean_rows,\n",
    "        \"after_dedup\": dedup_rows,\n",
    "        \"final_output\": final_rows\n",
    "    })\n",
    "\n",
    "    # ---------- Write ----------\n",
    "    if final_rows > 0:\n",
    "        df_out.coalesce(4).write.mode(\"append\").parquet(OUTBOX)\n",
    "        wrote = final_rows\n",
    "    else:\n",
    "        wrote = 0\n",
    "\n",
    "    # ---------- Runtime ----------\n",
    "    end = time.time()\n",
    "    print(\"Full job runtime (seconds):\", round(end - start, 2))\n",
    "    print(\"Rows written:\", wrote)\n",
    "\n",
    "    # ---------- Manifest update (always) ----------\n",
    "    for f in new_files:\n",
    "        file_path = os.path.join(INBOX, f)\n",
    "        manifest[\"processed_files\"].append({\n",
    "            \"filename\": f,\n",
    "            \"size_bytes\": os.path.getsize(file_path),\n",
    "            \"processed_at\": now\n",
    "        })\n",
    "\n",
    "    save_manifest(STATE, manifest)\n",
    "\n",
    "    print(\"✅ Run complete!\")\n",
    "    print(\"Output path:\", OUTBOX)\n",
    "    print(\"Manifest updated:\", STATE)\n",
    "    print(\"Cleaning rules: non-null required fields, passenger_count > 0, trip_distance > 0, dropoff > pickup\")\n",
    "    print(\"Dedup key:\", dedup_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c79f0c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bad_rows \u001b[38;5;241m=\u001b[39m \u001b[43mdf_new\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m      2\u001b[0m     (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassenger_count\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m|\u001b[39m\n\u001b[1;32m      3\u001b[0m     (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrip_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m|\u001b[39m\n\u001b[1;32m      4\u001b[0m     (col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_pickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m bad_rows\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassenger_count\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrip_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_pickup_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m )\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m3\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
     ]
    }
   ],
   "source": [
    "bad_rows = df_typed.filter(\n",
    "    (col(\"passenger_count\") <= 0) |\n",
    "    (col(\"trip_distance\") <= 0) |\n",
    "    (col(\"tpep_dropoff_datetime\") <= col(\"tpep_pickup_datetime\"))\n",
    ")\n",
    "\n",
    "bad_rows.select(\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\"\n",
    ").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cca5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = spark.read.parquet(OUTBOX)\n",
    "df_final.show(5, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
