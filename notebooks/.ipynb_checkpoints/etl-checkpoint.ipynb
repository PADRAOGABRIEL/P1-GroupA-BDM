{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e3c9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports used across ETL exploration and final incremental pipeline\n",
    "import time\n",
    "import os, json, shutil\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.sql.functions import col, current_timestamp, input_file_name, to_date, unix_timestamp, broadcast, desc, when, lit, concat, floor, rand, regexp_replace, sum as Fsum\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35041a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centralized project paths for input, output, lookup, and ETL state\n",
    "BASE_PATH = \"/home/jovyan/work\"\n",
    "\n",
    "INBOX = f\"{BASE_PATH}/data/inbox\"\n",
    "OUTBOX = f\"{BASE_PATH}/data/outbox/trips_enriched.parquet\"\n",
    "LOOKUP = f\"{BASE_PATH}/data/taxi_zone_lookup.parquet\"\n",
    "STATE = f\"{BASE_PATH}/state/manifest.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68770396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read manifest file that tracks already processed input files\n",
    "def load_manifest(path):\n",
    "    if not os.path.exists(path):\n",
    "        return {\"processed_files\": []}\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13f1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist updated manifest to keep ingestion incremental and idempotent\n",
    "def save_manifest(path, manifest):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192bf9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['yellow_tripdata_2025-01.parquet', 'yellow_tripdata_2025-02.parquet'],\n",
       " ['yellow_tripdata_2025-01.parquet', 'yellow_tripdata_2025-02.parquet'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute full input list and the subset of files that are new in this run\n",
    "manifest = load_manifest(STATE)\n",
    "processed = set(x[\"filename\"] for x in manifest[\"processed_files\"])\n",
    "\n",
    "all_files = sorted([f for f in os.listdir(INBOX) if f.endswith(\".parquet\")])\n",
    "new_files = [f for f in all_files if f not in processed]\n",
    "\n",
    "all_files, new_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd55059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session for the ETL run\n",
    "spark = SparkSession.builder.appName(\"Project1-ETL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddbb63",
   "metadata": {},
   "source": [
    "## ETL rules used in the final run\n",
    "\n",
    "This cell documents the business rules implemented in the final incremental Spark job.\n",
    "\n",
    "- **Cleaning rules**: keep only rows where required fields are non-null, `passenger_count > 0`, `trip_distance > 0`, and `tpep_dropoff_datetime > tpep_pickup_datetime`.\n",
    "- **Dedup key**: `VendorID`, `tpep_pickup_datetime`, `tpep_dropoff_datetime`, `PULocationID`, `DOLocationID`.\n",
    "- **Incremental behavior**: process only files not present in `state/manifest.json`.\n",
    "- **Idempotency**: anti-join new batch against existing outbox using the dedup key, then append only unseen records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac1451c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to verify whether output parquet folder already contains data files\n",
    "def outbox_has_data(path: str) -> bool:\n",
    "    return (\n",
    "        os.path.exists(path)\n",
    "        and os.path.isdir(path)\n",
    "        and any(name.startswith(\"part-\") and name.endswith(\".parquet\") for name in os.listdir(path))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a795b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup safety: remove empty/corrupted outbox folder before writing\n",
    "if os.path.exists(OUTBOX) and not outbox_has_data(OUTBOX):\n",
    "    print(\"OUTBOX exists but has no parquet parts. Deleting corrupted/empty outbox folder...\")\n",
    "    shutil.rmtree(OUTBOX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ed6132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files: ['yellow_tripdata_2025-01.parquet', 'yellow_tripdata_2025-02.parquet']\n",
      "New files: ['yellow_tripdata_2025-01.parquet', 'yellow_tripdata_2025-02.parquet']\n"
     ]
    }
   ],
   "source": [
    "# Log file discovery result for this run (all vs new files)\n",
    "print(\"All files:\", all_files)\n",
    "print(\"New files:\", new_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb47438c-c65a-4129-9ae4-439a6d2f0f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row counts (new batch): {'input': 7052769, 'after_cleaning': 5579927, 'after_dedup': 5486133, 'final_output': 5486133}\n",
      "Full job runtime (seconds): 35.77\n",
      "Rows written: 5486133\n",
      "âœ… Run complete!\n",
      "Output path: /home/jovyan/work/data/outbox/trips_enriched.parquet\n",
      "Manifest updated: /home/jovyan/work/state/manifest.json\n",
      "Cleaning rules: non-null required fields, passenger_count > 0, trip_distance > 0, dropoff > pickup\n",
      "Dedup key: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID']\n"
     ]
    }
   ],
   "source": [
    "# Final incremental ETL: process only new files, clean, dedup, enrich, append, and update manifest\n",
    "if not new_files:\n",
    "    print(\"Nothing new to process. Exiting without changes.\")\n",
    "else:\n",
    "    start = time.time()\n",
    "    now = datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "    new_paths = [os.path.join(INBOX, f) for f in new_files]\n",
    "    df_new = spark.read.parquet(*new_paths).withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "    # ---------- Types ----------\n",
    "    df_typed = (\n",
    "        df_new\n",
    "        .withColumn(\"tpep_pickup_datetime\", col(\"tpep_pickup_datetime\").cast(\"timestamp\"))\n",
    "        .withColumn(\"tpep_dropoff_datetime\", col(\"tpep_dropoff_datetime\").cast(\"timestamp\"))\n",
    "        .withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"int\"))\n",
    "        .withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"double\"))\n",
    "        .withColumn(\"PULocationID\", col(\"PULocationID\").cast(\"int\"))\n",
    "        .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(\"int\"))\n",
    "        .withColumn(\"VendorID\", col(\"VendorID\").cast(\"int\"))\n",
    "    )\n",
    "\n",
    "    # ---------- Cleaning ----------\n",
    "    df_clean = df_typed.filter(\n",
    "        col(\"tpep_pickup_datetime\").isNotNull()\n",
    "        & col(\"tpep_dropoff_datetime\").isNotNull()\n",
    "        & col(\"passenger_count\").isNotNull()\n",
    "        & col(\"trip_distance\").isNotNull()\n",
    "        & col(\"PULocationID\").isNotNull()\n",
    "        & col(\"DOLocationID\").isNotNull()\n",
    "        & col(\"VendorID\").isNotNull()\n",
    "        & (col(\"passenger_count\") > 0)\n",
    "        & (col(\"trip_distance\") > 0)\n",
    "        & (col(\"tpep_dropoff_datetime\") > col(\"tpep_pickup_datetime\"))\n",
    "    )\n",
    "\n",
    "    # ---------- Dedup ----------\n",
    "    dedup_key = [\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"PULocationID\", \"DOLocationID\"]\n",
    "\n",
    "    # 1) Remove duplicates within the current batch\n",
    "    df_dedup_batch = df_clean.dropDuplicates(dedup_key)\n",
    "\n",
    "   # 2) Optionally remove records that already exist in OUTBOX (ensures idempotency even without relying solely on the manifest)\n",
    "    if outbox_has_data(OUTBOX):\n",
    "        existing_keys = (\n",
    "            spark.read.parquet(OUTBOX)\n",
    "            .select(*dedup_key)\n",
    "            .dropDuplicates(dedup_key)\n",
    "        )\n",
    "        df_dedup = df_dedup_batch.join(existing_keys, on=dedup_key, how=\"left_anti\")\n",
    "    else:\n",
    "        df_dedup = df_dedup_batch\n",
    "\n",
    "    # ---------- Enrichment ----------\n",
    "    zones = spark.read.parquet(LOOKUP)\n",
    "\n",
    "    zones_pickup = zones.select(\n",
    "        col(\"LocationID\").alias(\"PULocationID\"),\n",
    "        col(\"Zone\").alias(\"pickup_zone\")\n",
    "    )\n",
    "\n",
    "    zones_dropoff = zones.select(\n",
    "        col(\"LocationID\").alias(\"DOLocationID\"),\n",
    "        col(\"Zone\").alias(\"dropoff_zone\")\n",
    "    )\n",
    "\n",
    "    df_enriched = (\n",
    "        df_dedup\n",
    "        .join(broadcast(zones_pickup), on=\"PULocationID\", how=\"left\")\n",
    "        .join(broadcast(zones_dropoff), on=\"DOLocationID\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # ---------- Derived columns ----------\n",
    "    df_ready = (\n",
    "        df_enriched\n",
    "        .withColumn(\n",
    "            \"trip_duration_minutes\",\n",
    "            (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 60.0\n",
    "        )\n",
    "        .withColumn(\"pickup_date\", to_date(\"tpep_pickup_datetime\"))\n",
    "        .withColumn(\"ingested_at\", current_timestamp())\n",
    "    )\n",
    "\n",
    "    # ---------- Select required output columns (helps grading) ----------\n",
    "    df_out = df_ready.select(\n",
    "        \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "        \"PULocationID\", \"DOLocationID\",\n",
    "        \"pickup_zone\", \"dropoff_zone\",\n",
    "        \"passenger_count\", \"trip_distance\",\n",
    "        \"trip_duration_minutes\", \"pickup_date\",\n",
    "        \"source_file\", \"ingested_at\"\n",
    "    )\n",
    "\n",
    "    # ---------- Metrics for README ----------\n",
    "    input_rows = df_new.count()\n",
    "    clean_rows = df_clean.count()\n",
    "    dedup_rows = df_dedup.count()\n",
    "    final_rows = df_out.count()\n",
    "\n",
    "    print(\"Row counts (new batch):\", {\n",
    "        \"input\": input_rows,\n",
    "        \"after_cleaning\": clean_rows,\n",
    "        \"after_dedup\": dedup_rows,\n",
    "        \"final_output\": final_rows\n",
    "    })\n",
    "\n",
    "    # ---------- Write ----------\n",
    "    if final_rows > 0:\n",
    "        df_out.coalesce(4).write.mode(\"append\").parquet(OUTBOX)\n",
    "        wrote = final_rows\n",
    "    else:\n",
    "        wrote = 0\n",
    "\n",
    "    # ---------- Runtime ----------\n",
    "    end = time.time()\n",
    "    print(\"Full job runtime (seconds):\", round(end - start, 2))\n",
    "    print(\"Rows written:\", wrote)\n",
    "\n",
    "    # ---------- Manifest update (always) ----------\n",
    "    for f in new_files:\n",
    "        file_path = os.path.join(INBOX, f)\n",
    "        manifest[\"processed_files\"].append({\n",
    "            \"filename\": f,\n",
    "            \"size_bytes\": os.path.getsize(file_path),\n",
    "            \"processed_at\": now\n",
    "        })\n",
    "\n",
    "    save_manifest(STATE, manifest)\n",
    "\n",
    "    print(\"Run complete!\")\n",
    "    print(\"Output path:\", OUTBOX)\n",
    "    print(\"Manifest updated:\", STATE)\n",
    "    print(\"Cleaning rules: non-null required fields, passenger_count > 0, trip_distance > 0, dropoff > pickup\")\n",
    "    print(\"Dedup key:\", dedup_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108c588-f787-4a36-9d42-f5827ce827a9",
   "metadata": {},
   "source": [
    "# Bad rows Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8cd031e-8fbb-4134-a653-7af29cf1fc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+---------------------+\n",
      "|passenger_count|trip_distance|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+---------------+-------------+--------------------+---------------------+\n",
      "|1              |0.0          |2025-01-01 00:49:48 |2025-01-01 00:49:48  |\n",
      "|1              |0.0          |2025-01-01 00:37:43 |2025-01-01 00:37:53  |\n",
      "|3              |0.0          |2025-01-01 00:57:08 |2025-01-01 00:57:16  |\n",
      "+---------------+-------------+--------------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trip distance <= 0\n",
    "bad_rows = df_typed.filter(\n",
    "    (col(\"trip_distance\") <= 0)\n",
    ")\n",
    "\n",
    "bad_rows.select(\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\"\n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14998839-3f68-4774-ac88-66ac7bfd5d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+---------------------+\n",
      "|passenger_count|trip_distance|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+---------------+-------------+--------------------+---------------------+\n",
      "|1              |9.0          |2025-01-02 12:26:00 |2025-01-02 11:29:58  |\n",
      "|1              |3.8          |2025-01-06 16:00:00 |2025-01-06 15:05:30  |\n",
      "|1              |1.0          |2025-01-15 15:00:00 |2025-01-15 14:42:48  |\n",
      "+---------------+-------------+--------------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropoff time < Pickup time\n",
    "\n",
    "bad_rows = df_typed.filter(\n",
    "    (col(\"tpep_dropoff_datetime\") < col(\"tpep_pickup_datetime\"))\n",
    ")\n",
    "\n",
    "bad_rows.select(\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\"\n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c79f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+--------------------+---------------------+\n",
      "|passenger_count|trip_distance|tpep_pickup_datetime|tpep_dropoff_datetime|\n",
      "+---------------+-------------+--------------------+---------------------+\n",
      "|0              |0.4          |2025-01-01 00:14:47 |2025-01-01 00:16:15  |\n",
      "|0              |1.6          |2025-01-01 00:39:27 |2025-01-01 00:51:51  |\n",
      "|0              |2.8          |2025-01-01 00:53:43 |2025-01-01 01:13:23  |\n",
      "+---------------+-------------+--------------------+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Passenger Count <= 0\n",
    "\n",
    "bad_rows = df_typed.filter(\n",
    "    (col(\"passenger_count\") <= 0)\n",
    ")\n",
    "\n",
    "bad_rows.select(\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\"\n",
    ").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de1ecb-a42f-489d-b762-b40bd307234d",
   "metadata": {},
   "source": [
    "# Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4cca5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+------------+------------+-------------------------+--------------+---------------+-------------+---------------------+-----------+-------------------------------------------------------------------+--------------------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|PULocationID|DOLocationID|pickup_zone              |dropoff_zone  |passenger_count|trip_distance|trip_duration_minutes|pickup_date|source_file                                                        |ingested_at               |\n",
      "+--------------------+---------------------+------------+------------+-------------------------+--------------+---------------+-------------+---------------------+-----------+-------------------------------------------------------------------+--------------------------+\n",
      "|2025-01-01 03:38:57 |2025-01-01 04:05:11  |230         |1           |Times Sq/Theatre District|Newark Airport|4              |17.91        |26.233333333333334   |2025-01-01 |file:///home/jovyan/work/data/inbox/yellow_tripdata_2025-01.parquet|2026-02-26 12:33:25.794385|\n",
      "|2025-01-01 03:39:52 |2025-01-01 04:10:36  |170         |1           |Murray Hill              |Newark Airport|1              |17.33        |30.733333333333334   |2025-01-01 |file:///home/jovyan/work/data/inbox/yellow_tripdata_2025-01.parquet|2026-02-26 12:33:25.794385|\n",
      "|2025-01-01 07:28:57 |2025-01-01 07:47:16  |45          |1           |Chinatown                |Newark Airport|2              |12.09        |18.316666666666666   |2025-01-01 |file:///home/jovyan/work/data/inbox/yellow_tripdata_2025-01.parquet|2026-02-26 12:33:25.794385|\n",
      "|2025-01-01 07:43:59 |2025-01-01 08:10:26  |87          |1           |Financial District North |Newark Airport|1              |16.15        |26.45                |2025-01-01 |file:///home/jovyan/work/data/inbox/yellow_tripdata_2025-01.parquet|2026-02-26 12:33:25.794385|\n",
      "|2025-01-01 09:12:07 |2025-01-01 09:41:39  |162         |1           |Midtown East             |Newark Airport|3              |18.28        |29.533333333333335   |2025-01-01 |file:///home/jovyan/work/data/inbox/yellow_tripdata_2025-01.parquet|2026-02-26 12:33:25.794385|\n",
      "+--------------------+---------------------+------------+------------+-------------------------+--------------+---------------+-------------+---------------------+-----------+-------------------------------------------------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final = spark.read.parquet(OUTBOX)\n",
    "df_final.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3435ff-ffc6-4a6d-a9ec-6fb333e0b783",
   "metadata": {},
   "source": [
    "## Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "779c1276-3c71-49bc-9ad5-ad68ad220993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot zone: Upper East Side South\n",
      "Hot zone count: 286719\n",
      "Hot zone %: 5.23 %\n"
     ]
    }
   ],
   "source": [
    "# Identifying most frequent zone\n",
    "\n",
    "df_scn = spark.read.parquet(OUTBOX)\n",
    "\n",
    "zone_counts = df_scn.groupBy(\"pickup_zone\").count()\n",
    "\n",
    "total = df_scn.count()\n",
    "\n",
    "top1 = (\n",
    "    zone_counts\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .first()\n",
    ")\n",
    "\n",
    "hot_zone = top1[\"pickup_zone\"]\n",
    "hot_count = top1[\"count\"]\n",
    "\n",
    "print(\"Hot zone:\", hot_zone)\n",
    "print(\"Hot zone count:\", hot_count)\n",
    "print(\"Hot zone %:\", round(100 * hot_count / total, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98006c2-1764-4686-b913-9c61c175ff80",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d74a38b-cf32-41d5-b9dd-60b01530ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This triggers a shuffle (Exchange) because groupBy is a wide transformation.\n",
    "# Use Spark UI to capture baseline metrics (duration, shuffle write/read, spill).\n",
    "baseline_agg = df_scn.groupBy(\"pickup_zone\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7186dda8-e972-4e2b-ad20-e46f06c7d05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------+\n",
      "|pickup_zone                 |count |\n",
      "+----------------------------+------+\n",
      "|Upper East Side South       |286719|\n",
      "|Midtown Center              |283351|\n",
      "|Upper East Side North       |263608|\n",
      "|JFK Airport                 |250019|\n",
      "|Penn Station/Madison Sq West|208572|\n",
      "|Midtown East                |204407|\n",
      "|Times Sq/Theatre District   |202414|\n",
      "|Lincoln Square East         |184968|\n",
      "|LaGuardia Airport           |166738|\n",
      "|Midtown North               |165699|\n",
      "+----------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Action -> creates the baseline Spark job\n",
    "# (Run this and take Spark UI screenshots for \"scenario_baseline_job.png\" and \"scenario_baseline_stage.png\")\n",
    "baseline_agg.orderBy(desc(\"count\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e57235-3149-4bad-90a9-7c2c2997d6cd",
   "metadata": {},
   "source": [
    "## Without Most Popular Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83c9d111-86ae-4e48-ae14-0d9b104cf75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------+\n",
      "|pickup_zone                 |count |\n",
      "+----------------------------+------+\n",
      "|Midtown Center              |283351|\n",
      "|Upper East Side North       |263608|\n",
      "|JFK Airport                 |250019|\n",
      "|Penn Station/Madison Sq West|208572|\n",
      "|Midtown East                |204407|\n",
      "|Times Sq/Theatre District   |202414|\n",
      "|Lincoln Square East         |184968|\n",
      "|LaGuardia Airport           |166738|\n",
      "|Midtown North               |165699|\n",
      "|Union Sq                    |156884|\n",
      "+----------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This checks whether removing the most frequent key changes runtime / shuffle behavior.\n",
    "df_no_hot = df_scn.filter(col(\"pickup_zone\") != hot_zone)\n",
    "\n",
    "no_hot_agg = df_no_hot.groupBy(\"pickup_zone\").count()\n",
    "\n",
    "# Action -> creates a Spark job (take UI screenshots for \"scenario_wfz_job.png\" and \"scenario_wfz_stage.png\")\n",
    "no_hot_agg.orderBy(desc(\"count\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c245299-ff03-44b4-8423-d9d94c49ee0b",
   "metadata": {},
   "source": [
    "## Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcb3116a-0f43-4db2-9a2a-b008d6cae23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------+\n",
      "|pickup_zone                 |count |\n",
      "+----------------------------+------+\n",
      "|Upper East Side South       |286719|\n",
      "|Midtown Center              |283351|\n",
      "|Upper East Side North       |263608|\n",
      "|JFK Airport                 |250019|\n",
      "|Penn Station/Madison Sq West|208572|\n",
      "|Midtown East                |204407|\n",
      "|Times Sq/Theatre District   |202414|\n",
      "|Lincoln Square East         |184968|\n",
      "|LaGuardia Airport           |166738|\n",
      "|Midtown North               |165699|\n",
      "+----------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# repartition forces an extra shuffle.\n",
    "# It can help in severe skew cases (very dominant keys) by improving distribution,\n",
    "# but it may add overhead when skew is mild.\n",
    "repart_agg = (\n",
    "    df_scn\n",
    "    .repartition(\"pickup_zone\")      # Extra shuffle to redistribute rows by key\n",
    "    .groupBy(\"pickup_zone\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Action -> creates a Spark job (take UI screenshots for \"scenario_repart_job.png\" and \"scenario_repart_stage.png\")\n",
    "repart_agg.orderBy(desc(\"count\")).show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
